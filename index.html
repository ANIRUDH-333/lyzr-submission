<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Anirudh Edpuganti - Education Engineer Submission</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        /* Simple styling for the <details> summary to act as an accordion */
        summary {
            cursor: pointer;
            list-style: none;
            /* Remove default marker */
        }

        summary::-webkit-details-marker {
            display: none;
            /* Hide marker for Chrome/Safari */
        }

        summary:focus {
            outline: none;
        }

        /* Add a custom marker */
        summary:before {
            content: '►';
            margin-right: 0.5rem;
            font-size: 0.8em;
            transition: transform 0.2s ease-in-out;
        }

        details[open] summary:before {
            transform: rotate(90deg);
        }

        /* Style for analogies */
        .analogy {
            background-color: #fefce8;
            /* yellow-50 */
            border-left: 4px solid #facc15;
            /* yellow-400 */
            padding: 1rem;
            margin: 1rem 0;
            border-radius: 0.25rem;
        }

        .analogy-title {
            font-weight: 600;
            color: #ca8a04;
            /* yellow-600 */
            display: block;
            margin-bottom: 0.5rem;
        }

        /* Style for labs */
        .lab {
            background-color: #f0fdf4;
            /* green-50 */
            border-left: 4px solid #4ade80;
            /* green-400 */
            padding: 1rem;
            margin: 1rem 0;
            border-radius: 0.25rem;
        }

        .lab-title {
            font-weight: 600;
            color: #16a34a;
            /* green-600 */
            display: block;
            margin-bottom: 0.5rem;
        }

        /* Add Inter font */
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');

        body {
            font-family: 'Inter', sans-serif;
            background-color: #f8fafc;
            /* slate-50 */
            color: #334155;
            /* slate-700 */
        }
    </style>
</head>

<body class="antialiased">

    <div class="container mx-auto max-w-4xl p-5 md:p-10">

        <!-- Header -->
        <header class="mb-10">
            <h1 class="text-3xl md:text-4xl font-bold text-gray-900">Education Engineer Submission</h1>
            <p class="mt-2 text-lg text-gray-600">By Anirudh Edpuganti</p>
            <p class="mt-4 text-gray-700">Thank you for the opportunity. Below is my proposed 4-week curriculum for the
                "Agent Building for Developers" course (Part 1), along with my 3-minute teaching video (Part 2).</p>
        </header>

        <!-- Part 1: The 4-Week Curriculum -->
        <section class="mb-12">
            <h2 class="text-2xl font-semibold text-gray-900 mb-6">Part 1: 4-Week Course Curriculum</h2>

            <div class="space-y-4">

                <!-- Week 1 -->
                <details class="bg-white p-6 rounded-lg shadow-sm border border-gray-200">
                    <summary class="text-xl font-semibold text-gray-800">
                        Week 1: The Agentic Leap – From Prompting to Problem-Solving
                    </summary>
                    <div class="mt-4 prose prose-slate max-w-none text-gray-700">
                        <p class="text-lg"><strong>Weekly Goal:</strong> To "demystify" AI Agents by answering the
                            critical "Why?" and "What?". By the end of this week, you will move from being a developer
                            who uses an LLM to a developer who understands how to automate with an LLM. You will build
                            your first simple agent from scratch, with no magic frameworks.</p>

                        <hr class="my-4">

                        <h3 class="font-semibold text-lg text-gray-800">Module 1.1: Why Agents? The Shift from
                            Instruction to Intention</h3>
                        <p><strong>Concept:</strong> This module is our "Why?". Before we write any code, we must
                            establish why agents are the necessary next step. We'll explore the limitations of
                            single-call LLMs (like a standard ChatGPT prompt) for any complex, multi-step task. We are
                            moving from a world of giving instructions to a world of defining intent.</p>

                        <div class="analogy">
                            <span class="analogy-title">Developer Analogy: The REPL vs. The Ansible Script</span>
                            <p>A "one-shot" LLM call is like a <strong>REPL (Read-Eval-Print Loop)</strong>: You type
                                <code>2 + 2</code> and it returns <code>4</code>. You ask a question, it gives an
                                answer. It's a powerful, single-turn "Read-Eval-Print" cycle. But it's passive. It has
                                no state and cannot act on its own.
                            </p>
                            <p>An <strong>AI Agent</strong> is like an <strong>Ansible Playbook</strong> or a
                                <strong>Terraform Script</strong>: You don't tell it how to do every step. You give it a
                                goal (e.g., "ensure this server is configured," "get me a brief on Company X"). The
                                script then autonomously observes the current state, thinks about the steps needed, and
                                acts to achieve the desired state. It can run commands, check results, and loop until
                                the goal is met.
                            </p>
                        </div>

                        <div class="bg-blue-50 border-l-4 border-blue-400 p-4 my-4 rounded">
                            <h4 class="font-semibold text-blue-800 mb-2">Key Learning:</h4>
                            <ul class="list-disc pl-5 text-blue-700">
                                <li>LLMs are passive "brains" that respond to instructions.</li>
                                <li>Agents are active systems that execute on intent.</li>
                                <li>Agents are designed to automate complex, multi-step workflows that LLMs alone cannot
                                    handle.</li>
                            </ul>
                        </div>

                        <div class="lab">
                            <span class="lab-title">Hands-On Lab 1.1: The "Wow" Demo & The "Human Agent" Exercise</span>
                            <p><strong>Demo:</strong> We will show a side-by-side comparison of a "Manual" vs. "Agentic"
                                workflow (e.g., "Research the top 3 competitors for Lyzr AI and summarize their main
                                products"). We'll show the manual process (Googling, opening tabs, reading,
                                copy-pasting) vs. a single prompt to an agent that does the same.</p>
                            <p><strong>Exercise:</strong> You will be the agent. We'll give you a task: "Find the best
                                Python library for PDF parsing that was released in the last 6 months." You will write
                                down the exact, literal steps you would take (e.g., "1. Go to Google. 2. Search for 'new
                                python pdf library 2024'. 3. Open the top 5 links..."). This exercise forces you to see
                                the "program" an agent needs to run.</p>
                        </div>

                        <h3 class="font-semibold text-lg text-gray-800 mt-6">Module 1.2: Deconstructing the "Magic": An
                            Agent's Anatomy</h3>
                        <p><strong>Concept:</strong> We'll pull back the curtain and show that an agent is not a magical
                            black box. It's a simple, elegant programming loop that you already understand. We will
                            introduce the core loop of all agents: <strong>Observe, Think, Act (OTA)</strong>.</p>

                        <div class="analogy">
                            <span class="analogy-title">Developer Analogy: An Agent is Just a while Loop</span>
                            <p>An agent's "magic" is just a while loop that uses an LLM to decide what to do next.</p>
                            <pre class="bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto text-sm"><code># This is the core logic of almost every agent
goal = "What's the weather in London?"
history = [] # Our "state"
goal_achieved = False

while not goal_achieved:
    # 1. THINK: The LLM acts as a router or planner
    prompt = f"Given history: {history}, what's the next step to achieve: {goal}?"
    llm_response = llm.call(prompt) 
    
    # 2. ACT: We parse the LLM's response
    if llm_response.says_final_answer():
        print(llm_response.answer)
        goal_achieved = True
    
    elif llm_response.wants_to_use_tool("get_weather"):
        city = llm_response.get_parameter("city")
        
        # 3. OBSERVE: We run the tool and get the result
        weather_data = get_weather(city)
        history.append(f"Tool Result: {weather_data}")
        
    # The loop repeats, feeding the new observation back to the LLM</code></pre>
                        </div>

                        <div class="bg-blue-50 border-l-4 border-blue-400 p-4 my-4 rounded">
                            <h4 class="font-semibold text-blue-800 mb-2">Key Learning:</h4>
                            <ul class="list-disc pl-5 text-blue-700">
                                <li>The "Observe, Think, Act" (OTA) loop is the fundamental pattern of all agentic
                                    systems.</li>
                                <li>The LLM's role shifts from being an answerer to being a planner or router that
                                    decides the next step.</li>
                                <li>A "Thought" is just the LLM's reasoning, often in a structured format like JSON,
                                    that explains its plan and what action (or tool) to use next.</li>
                            </ul>
                        </div>

                        <div class="lab">
                            <span class="lab-title">Hands-On Lab 1.2: Pseudocode the OTA Loop</span>
                            <p><strong>Task:</strong> Using the "Human Agent" exercise from Lab 1.1 ("find the best
                                Python PDF library..."), you will now write the pseudocode for an agent that can
                                accomplish it.</p>
                            <p><strong>Goal:</strong> You must use the while loop structure. Define what your "tools"
                                would be (e.g., <code>google_search(query)</code>, <code>read_webpage(url)</code>) and
                                write the if/elif logic inside your loop to show how the agent would "Think" and "Act."
                            </p>
                        </div>

                        <h3 class="font-semibold text-lg text-gray-800 mt-6">Module 1.3: "Hello, Agent!" – Building Your
                            First Agent from Scratch</h3>
                        <p><strong>Concept:</strong> This is where the confidence is built. We will build a real,
                            working agent using only standard Python and an LLM API key. No frameworks, no libraries, no
                            magic. This lab proves that all the concepts from Modules 1.1 and 1.2 are just simple,
                            understandable code.</p>

                        <div class="analogy">
                            <span class="analogy-title">Developer Analogy: No Frameworks, No Magic</span>
                            <p>This is the "learn to build a web server with Python's basic <code>http.server</code>" of
                                agent development. Before you use a complex framework like Django or Flask, you must
                                understand the raw request/response cycle. By building this "from scratch," you will
                                know what the frameworks are doing for you later.</p>
                        </div>

                        <div class="bg-blue-50 border-l-4 border-blue-400 p-4 my-4 rounded">
                            <h4 class="font-semibold text-blue-800 mb-2">Key Learning:</h4>
                            <ul class="list-disc pl-5 text-blue-700">
                                <li>An agent is just a Python script with a loop and a list to manage history (state).
                                </li>
                                <li>A "Tool" is just a Python function you write and describe in the system prompt.</li>
                                <li>The System Prompt is the "operating system" for your agent. It defines the rules,
                                    the goal, and the "API" of the tools the agent is allowed to call.</li>
                            </ul>
                        </div>

                        <div class="lab">
                            <span class="lab-title">Hands-On Lab 1.3: The "No-Magic" Weather Agent</span>
                            <p><strong>Task:</strong> You will write a single Python script (<code>agent.py</code>) that
                                can answer the question: "What's the weather in London?"</p>

                            <div class="mt-3">
                                <h5 class="font-semibold text-green-700 mb-2">Requirements:</h5>
                                <ol class="list-decimal pl-5 space-y-2">
                                    <li>You will write a simple Python function <code>get_weather(city)</code>. (It can
                                        just return a hard-coded JSON string like
                                        <code>{"city": "London", "temp": "15C"}</code> for this lab).
                                    </li>
                                    <li>You will write a <code>system_prompt</code> that tells the LLM: "You are a
                                        helpful assistant. You have one tool: get_weather(city). When asked about the
                                        weather, you MUST reply only with a JSON object in the format: {"thought":
                                        "...", "action": "get_weather", "city": "..."}. If you have the final answer,
                                        reply with: {"thought": "...", "final_answer": "..."}"</li>
                                    <li>You will write the while loop that:
                                        <ul class="list-disc pl-5 mt-2 space-y-1">
                                            <li>Calls the LLM with the user query and prompt.</li>
                                            <li>Parses the LLM's JSON response.</li>
                                            <li>If it sees an "action", it calls your local <code>get_weather</code>
                                                function.</li>
                                            <li>It then takes the function's output and feeds it back into the LLM in
                                                the next loop.</li>
                                            <li>It continues until the LLM returns a <code>final_answer</code>.</li>
                                        </ul>
                                    </li>
                                </ol>
                            </div>
                        </div>
                    </div>
                </details>

                <!-- Week 2 -->
                <details class="bg-white p-6 rounded-lg shadow-sm border border-gray-200">
                    <summary class="text-xl font-semibold text-gray-800">
                        Week 2: Giving Agents Memory – The Power of RAG
                    </summary>
                    <div class="mt-4 prose prose-slate max-w-none text-gray-700">
                        <p class="text-lg"><strong>Weekly Goal:</strong> To solve the agent "amnesia problem." You will
                            learn the difference between short-term conversational memory and long-term knowledge
                            memory. By the end of this week, you will understand and build the most critical pattern in
                            agentic AI: <strong>Retrieval-Augmented Generation (RAG)</strong>, allowing your agents to
                            access and reason about external data (like your own documents).</p>

                        <hr class="my-4">

                        <h3 class="font-semibold text-lg text-gray-800">Module 2.1: The Amnesia Problem (Short-Term
                            Memory)</h3>
                        <p><strong>Concept:</strong> We'll start by defining the problem. LLMs are, by default,
                            stateless. Every API call is completely independent, like a curl request to a REST API. It
                            has no memory of past interactions. This is the "amnesia problem." We'll explore the first
                            and simplest solution: conversational history.</p>

                        <div class="analogy">
                            <span class="analogy-title">Developer Analogy: HTTP vs. Web Sockets (or Sessions)</span>
                            <p>A "stateless" LLM call is like an <strong>HTTP request</strong>: Every single request is
                                new. If you want the server to know who you are, you must pass your entire context (like
                                a JWT or session cookie) with every single call.</p>
                            <p>An agent with "short-term memory" is like a <strong>request.session</strong> object (or a
                                stateful WebSocket): We can store the conversation history in a list (like our history
                                list from Lab 1.3) and pass it back with each new request. This "fakes" a continuous
                                conversation.</p>
                        </div>

                        <div class="bg-blue-50 border-l-4 border-blue-400 p-4 my-4 rounded">
                            <h4 class="font-semibold text-blue-800 mb-2">Key Learning:</h4>
                            <ul class="list-disc pl-5 text-blue-700">
                                <li><strong>Problem:</strong> The history list will eventually grow too large and
                                    overflow the LLM's context window (its "RAM"). This is expensive, slow, and will
                                    eventually fail.</li>
                                <li><strong>Solution:</strong> Short-term memory is only for short conversations. We
                                    need a different solution for "long-term memory" (i.e., knowledge).</li>
                            </ul>
                        </div>

                        <div class="lab">
                            <span class="lab-title">Hands-On Lab 2.1: Breaking the Context Window</span>
                            <p><strong>Task:</strong> You will modify your "No-Magic Agent" from Week 1.</p>
                            <p><strong>Goal:</strong> We will simulate a long conversation by pre-filling the history
                                list with 10,000 words of "fake" conversational text.</p>
                            <p><strong>Result:</strong> When you run your agent, you will see the LLM API call fail with
                                a "Context Length Exceeded" error. This lab makes the abstract problem of the context
                                window a tangible, physical bug that you, the developer, must now solve.</p>
                        </div>

                        <h3 class="font-semibold text-lg text-gray-800 mt-6">Module 2.2: Long-Term Memory (Embeddings &
                            Vector Databases)</h3>
                        <p><strong>Concept:</strong> Now that we've proven our agent can't "remember" large files, we'll
                            introduce the solution: Vector Databases. To understand them, we must first understand
                            "embeddings."</p>

                        <div class="analogy">
                            <span class="analogy-title">Developer Analogy: A Git Commit Hash</span>
                            <p><strong>What is an Embedding?</strong> It's a way to turn complex data (like text) into a
                                machine-readable signature (a list of numbers, or "vector").</p>
                            <p><strong>The Analogy:</strong> How does Git know if a file has changed? It doesn't diff
                                the whole file every time. It runs the file content through a hash function (SHA-1) to
                                create a unique, fixed-size signature (the "hash").</p>
                            <pre
                                class="bg-gray-100 p-3 rounded text-sm"><code>"Hello, World!" -> a591a6d40bf420404a011733cfb7b190d62c65bf</code></pre>
                            <p>An <strong>Embedding Model</strong> is just a "hash function" for meaning (semantics). It
                                turns text into a "semantic hash" (a vector) so that similar-meaning texts have
                                similar-looking vectors.</p>
                        </div>

                        <div class="analogy mt-4">
                            <span class="analogy-title">Developer Analogy: SQL LIKE vs. Vector Search</span>
                            <p>A <strong>SQL DB</strong> finds literal matches. If you search for "developer," it will
                                not find "software engineer."</p>
                            <pre
                                class="bg-gray-100 p-3 rounded text-sm mt-2"><code>SELECT * FROM docs WHERE content LIKE '%developer%';</code></pre>
                            <p>A <strong>Vector DB</strong> finds conceptual or semantic matches. If you search for
                                "developer," it will find "software engineer," "coder," and "programmer" because their
                                "semantic hashes" (vectors) are numerically close in the database.</p>
                        </div>

                        <div class="lab">
                            <span class="lab-title">Hands-On Lab 2.2: "Hello, Vectors!"</span>
                            <p><strong>Task:</strong> We'll use a simple, in-memory vector DB library (like
                                <code>chromadb</code>).</p>
                            <p><strong>Goal:</strong> Prove the concept of semantic search in a few lines of Python.</p>
                            <pre class="bg-gray-900 text-gray-100 p-4 rounded-lg overflow-x-auto text-sm mt-3"><code>db.add(document="The cat is sleeping on the couch.", id="doc1")
db.add(document="The dog is barking in the yard.", id="doc2")
db.add(document="I love to code in Python.", id="doc3")

# Run a query
results = db.query(query_text="What is the feline doing?")</code></pre>
                            <p><strong>Result:</strong> The database will return <code>doc1</code>. This is the "aha!"
                                moment. You searched for "feline" and it found "cat." You now have a database that can
                                search by meaning.</p>
                        </div>

                        <h3 class="font-semibold text-lg text-gray-800 mt-6">Module 2.3: The "Chat with your Docs" Agent
                            (RAG)</h3>
                        <p><strong>Concept:</strong> This is the payoff. We will now combine our agent loop (Week 1)
                            with our new vector database (Module 2.2) to build a <strong>Retrieval-Augmented Generation
                                (RAG)</strong> agent. This is the pattern for building agents that can "talk to" private
                            data.</p>

                        <div class="analogy">
                            <span class="analogy-title">Developer Analogy: The "Open-Book Exam"</span>
                            <p>A normal LLM call is a "closed-book exam." The LLM can only answer based on what it was
                                trained on (what it "memorized" up to 2023).</p>
                            <p>A <strong>RAG call</strong> is an "open-book exam." The agent isn't asked to know the
                                answer. It's asked to find the answer and summarize it.</p>
                        </div>

                        <div class="bg-purple-50 border-l-4 border-purple-400 p-4 my-4 rounded">
                            <h4 class="font-semibold text-purple-800 mb-3">The RAG Flow (This is the most important
                                part):</h4>
                            <div class="space-y-3 text-purple-700">
                                <div><strong>User Query:</strong> "What is Lyzr's AgentMesh?"</div>
                                <div>
                                    <strong>Step 1: Retrieve (The SELECT):</strong> The agent does not ask the LLM. It
                                    first asks the Vector DB.
                                    <pre
                                        class="bg-purple-100 p-2 rounded text-sm mt-1"><code>context_chunks = db.query("What is Lyzr's AgentMesh?")</code></pre>
                                </div>
                                <div><strong>Step 2: Augment (The "Context Stuffing"):</strong> The agent takes the user
                                    query and stuffs the retrieved text chunks into a new prompt.</div>
                                <div><strong>Step 3: Generate (The "Render"):</strong> The agent finally calls the LLM
                                    with this new, augmented prompt.</div>
                            </div>
                        </div>

                        <div class="bg-gray-50 p-4 rounded-lg my-4">
                            <h5 class="font-semibold text-gray-800 mb-2">Example Augmented Prompt:</h5>
                            <pre class="bg-gray-900 text-gray-100 p-4 rounded text-sm overflow-x-auto"><code>"You are a helpful assistant. Using ONLY the context below, answer the user's question.

Context:
- 'AgentMesh is a protocol...'
- 'It allows for agent-to-agent communication...'

User's Question:
What is Lyzr's AgentMesh?"</code></pre>
                        </div>

                        <div class="lab">
                            <span class="lab-title">Hands-On Lab 2.3: "Chat with Lyzr" RAG Agent</span>
                            <p><strong>Task:</strong> You will build a complete RAG agent from scratch.</p>

                            <div class="mt-3">
                                <h5 class="font-semibold text-green-700 mb-2">Requirements:</h5>
                                <ol class="list-decimal pl-5 space-y-2">
                                    <li>We will provide a <code>lyzr_about.txt</code> file.</li>
                                    <li>Your script will first <strong>index the file</strong>: read it, split it into
                                        chunks, embed them, and save them to a chromadb store.</li>
                                    <li>Your script will then run the <strong>RAG loop</strong>:
                                        <ul class="list-disc pl-5 mt-2 space-y-1">
                                            <li>Take a user's question (e.g., "What super agents does Lyzr have?").</li>
                                            <li><strong>Retrieve</strong> the relevant chunks from the DB.</li>
                                            <li><strong>Augment</strong> the prompt with those chunks.</li>
                                            <li><strong>Generate</strong> the final answer from the LLM.</li>
                                        </ul>
                                    </li>
                                </ol>
                                <p class="mt-3"><strong>Result:</strong> A working "Chat with your Docs" agent. You have
                                    solved the memory problem and built the most in-demand agentic system.</p>
                            </div>
                        </div>
                    </div>
                </details>

                <!-- Week 3 -->
                <details class="bg-white p-6 rounded-lg shadow-sm border border-gray-200">
                    <summary class="text-xl font-semibold text-gray-800">
                        Week 3: Building an Agent Team – Multi-Agent Orchestration
                    </summary>
                    <div class="mt-4 prose prose-slate max-w-none text-gray-700">
                        <p class="text-lg"><strong>Weekly Goal:</strong> To scale our systems by moving from a single
                            "God" agent to a collaborative team of specialized agents. You will learn the core patterns
                            of multi-agent design and orchestrate a workflow, setting the stage for building true "super
                            agents."</p>

                        <hr class="my-4">

                        <h3 class="font-semibold text-lg text-gray-800">Module 3.1: The "God Agent" Problem</h3>
                        <p><strong>Concept:</strong> We'll show that as you add more tools and logic to a single agent,
                            its prompt becomes a "God prompt"—a bloated, brittle, and unmaintainable mess.</p>
                        <div class="analogy">
                            <span class="analogy-title">Developer Analogy: The Monolith vs. Microservices</span>
                            <p>A "God Agent" is a <strong>Monolith</strong>. It's one giant `main.py` that handles auth,
                                DB, email, and business logic. A <strong>Multi-Agent System</strong> is a
                                <strong>Microservices Architecture</strong>. We create specialized agents (a
                                `ResearchAgent`, a `WritingAgent`, an `EmailAgent`) that each do one thing perfectly.
                                This is modular, testable, and scalable.
                            </p>
                        </div>

                        <h3 class="font-semibold text-lg text-gray-800 mt-6">Module 3.2: The Conductor and the Orchestra
                            (Orchestration Patterns)</h3>
                        <p><strong>Concept:</strong> Now that we have a team, we need a "conductor" (Orchestrator
                            Agent). We'll cover the two patterns Lyzr uses: Managerial (Hierarchical) and DAG
                            (Sequential).</p>
                        <div class="analogy">
                            <span class="analogy-title">Developer Analogies for Orchestration</span>
                            <p><strong>1. The "Managerial" (Hierarchical) Model:</strong> This is a <strong>Tech Lead
                                    and Junior Devs</strong>. The Manager Agent (Lead) gets a complex goal, breaks it
                                down, and delegates tasks to worker agents (Juniors), then reviews their work and plans
                                the next step. It's dynamic and great for complex, unknown problems.</p>
                            <p><strong>2. The "DAG" (Sequential) Model:</strong> This is a <strong>CI/CD Pipeline
                                    (`.gitlab-ci.yml`)</strong>. It's a fixed, predictable workflow (`lint` -> `test` ->
                                `build`). The output of one agent is "piped" directly to the input of the next. It's
                                reliable and perfect for auditable business processes.</p>
                        </div>

                        <h3 class="font-semibold text-lg text-gray-800 mt-6">Module 3.3: "Hello, Team!" – Building a
                            3-Agent Workflow</h3>
                        <p><strong>Concept:</strong> We'll build a "blog post" generator using the **DAG (Sequential)
                            Model** because it's the easiest to build from scratch and is the agentic equivalent of a
                            `bash` pipe.</p>
                        <div class="lab">
                            <span class="lab-title">Hands-On Lab 3.3: The "Assembly Line" Agent Team</span>
                            <p><strong>Task:</strong> Build a 3-agent DAG workflow (`research_agent.py`,
                                `writing_agent.py`, `review_agent.py`).</p>
                            <p><strong>Goal:</strong> Your main script will "pipe" the output of the first agent into
                                the second, and the second into the third. We'll end by posing a question: "This works,
                                but now 5 agents all need to know about our 10 tools. How do we manage that?" This is
                                the "N x M" problem that leads perfectly into Week 4.</p>
                        </div>
                    </div>
                </details>

                <!-- Week 4 -->
                <details class="bg-white p-6 rounded-lg shadow-sm border border-gray-200">
                    <summary class="text-xl font-semibold text-gray-800">
                        Week 4: The Lyzr Deep Dive – From Prototype to Production "Super Agent"
                    </summary>
                    <div class="mt-4 prose prose-slate max-w-none text-gray-700">
                        <p class="text-lg"><strong>Weekly Goal:</strong> To bridge the gap from a "from-scratch"
                            prototype to a scalable, production-grade agentic system. You will solve the "N x M" tool
                            integration problem using the Model Context Protocol (MCP) and synthesize all your skills
                            into a capstone project: building a "super agent" modeled on Lyzr's `Diane for HR`.</p>

                        <hr class="my-4">

                        <h3 class="font-semibold text-lg text-gray-800">Module 4.1: The "Integration Hell" Problem (The
                            "N x M" Problem)</h3>
                        <p><strong>Concept:</strong> We formalize the "N x M" problem (20 agents needing to know about
                            10 tools). Copy-pasting 10 tool definitions into 20 system prompts is unmaintainable. This
                            is the "Integration Hell" that stops prototypes from becoming products.</p>
                        <div class="analogy">
                            <span class="analogy-title">Developer Analogy: Hard-Coded `fetch` vs. a Service Mesh
                                (gRPC)</span>
                            <p>Our "from-scratch" way is like hard-coding `fetch('http://weather-api.com')` inside 20
                                microservices. The **"Production" Way (MCP)** is like a <strong>Service Mesh
                                    (gRPC/Istio)</strong>. You build your tool *once* as an independent "service." All
                                your agents ("clients") just connect to the mesh, which handles <strong>service
                                    discovery</strong>. The agent just says "I need the 'weather' service," and the mesh
                                connects them. This is what the **Model Context Protocol (MCP)** does for AI agents.</p>
                        </div>
                        <p><strong>Key Learning:</strong> MCP is an open standard that **decouples agents from their
                            tools.** Tools become "MCP Servers," and agents become "MCP Clients."</This>

                        <h3 class="font-semibold text-lg text-gray-800 mt-6">Module 4.2: Refactoring Our Team with MCP
                        </h3>
                        <p><strong>Concept:</strong> A "before and after" refactoring lab. We'll take our messy,
                            hard-coded "Assembly Line" from Lab 3.3 and refactor it to use the MCP standard, showing how
                            it solves the N x M problem.</p>
                        <div class="lab">
                            <span class="lab-title">Hands-On Lab 4.2: The "Service Mesh" Refactor</span>
                            <p><strong>Task:</strong> You will refactor your 3-agent workflow from Week 3.</p>
                            <p><strong>"Before":</strong> `research_agent.py` has the `Google Search` tool hard-coded in
                                its prompt.</p>
                            <p><strong>"After":</strong> We'll provide a `tool_server.py` that runs an MCP Server
                                exposing the `Google Search` tool. You will *delete* the tool definition from your
                                agent's prompt and modify it to be an MCP Client. Your agent's prompt is now clean, and
                                *any* agent can now use the `Google Search` tool for free. The "N x M" problem is
                                solved.</p>
                        </div>

                        <h3 class="font-semibold text-lg text-gray-800 mt-6">Module 4.3: Capstone Project – Building
                            "Diane-Lite" (An HR Super Agent)</h3>
                        <p><strong>Concept:</strong> The final project. You will synthesize *everything*—the agent loop,
                            RAG, multi-agent orchestration, and MCP—to build a simplified version of Lyzr's `Diane for
                            HR` super agent. This will use the "Managerial" (Hierarchical) model.</p>

                        <p><strong>The Architecture:</strong></p>
                        <ul class="list-disc pl-5">
                            <li><strong>`Diane-Manager` (The Conductor):</strong> The user only talks to this agent. It
                                takes the goal, breaks it down, and delegates.</li>
                            <li><strong>`HR-Policy-Agent` (Worker):</strong> A RAG agent (from Week 2) connected to
                                `hr_policy.pdf`.</li>
                            <li><strong>`Resume-Screener-Agent` (Worker):</strong> A RAG agent connected to a `resumes/`
                                folder.</li>
                            <li><strong>`Calendar-Agent` (Worker):</strong> An agent with a tool to
                                `find_available_interview_slot()`.</li>
                        </ul>

                        <div class="lab">
                            <span class="lab-title">Hands-On Lab 4.3: The "Super Agent" Workflow</span>
                            <p><strong>Task:</strong> Assemble all four agents and run a final "integration test" with a
                                single, complex user prompt:</p>
                            <p><em>"Hi, I'm hiring a 'Senior Python Developer.' First, what is our company's policy on
                                    remote work? Second, please find the top 3 candidates for that role from our resume
                                    folder and book a 1-hour interview slot for them next Tuesday."</em></p>
                            <p><strong>The "Aha!" Moment:</strong> You will watch the `Diane-Manager`'s "thoughts" as it
                                intelligently orchestrates the entire workflow: calling the policy agent, then the
                                resume agent, then the calendar agent, and finally synthesizing all three results into a
                                single, perfect answer for the user.</p>
                        </div>

                        <h3 class="font-semibold text-lg text-gray-800 mt-6">Module 4.4: The Road Ahead</h3>
                        <p><strong>Concept:</strong> A final wrap-up on production readiness.</p>
                        <ul class="list-disc pl-5">
                            <li><strong>Evaluation:</strong> How do you test an agent? We'll discuss "AI-as-a-grader"
                                (using one LLM to "grade" another's output).</li>
                            <li><strong>Safety & Guardrails:</strong> The importance of "human-in-the-loop" approval
                                steps before an agent can *act* (e.g., send an email, modify a database).</li>
                            <li><strong>The Future:</strong> An inspiring look at the "Agent Movement."</li>
                        </ul>
                    </div>
                </details>
            </div>

        </section>

        <!-- Part 2: The Video -->
        <section>
            <h2 class="text-2xl font-semibold text-gray-900 mb-4">Part 2: 3-Minute Teaching Video</h2>
            <p class="mb-4">For the video portion, I chose to teach **Module 2.3: The "Chat with your Docs" Agent
                (RAG)**. This concept is fundamental to building useful agents and provides a great "aha!" moment for
                developers.</p>
            <div class="aspect-w-16 aspect-h-9 rounded-lg overflow-hidden shadow-xl border border-gray-200">
                <!-- 
    </div>

</body>
</html>